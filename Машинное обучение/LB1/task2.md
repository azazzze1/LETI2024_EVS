# Задание 2

## Статья
Название: Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT

Для цитирования: Hagendorff T., Fabi S., Kosinski M. Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT //Nature Computational Science. – 2023. – Т. 3. – №. 10. – С. 833-838.

Авторы: Thilo Hagendorff, Sarah Fabi & Michal Kosinski.

URL: https://www.nature.com/articles/s43588-023-00527-x

##Характеристика используемых данных

####Методы проведения эксперимента
Для выявления когнитивных ошибок у людей и языковых моделей авторы сгенерировали выборку задач. Данная выборка является ключевым датасетом, на которой тестируются и оцениваются ответы испытуемых. Выборка имеет тектовый тип с разметкой данных по классам CRT1, CRT2, CRT3 и Semantic Illusion. Задачи типа CRT1 и CRT2 были сгенерированы автоматически. Для получения задач Semantic illusion и CRT3 исследователи наняли ассистентов с фриланс-проекта Upwork, не уведомляя их о поставленной гипотезе. Всего задач каждого типа 50.

####Используемые данные и их характеристики
Задачи типа **CRT1** - задача, которая словами "больше, чем" заставляет испытуемых вычитать два значения, вместо поиска более сложной связи. Задача **CRT2** - задача, которая ловит испытуемых на аналогиях и ситуациях, которые кажутся поначалу очень схожими. Задача **CRT3** - задача, которая заставляет испытуемых воспринимать сложную зависимость, как линейную. Задача **Semantic Illusion** - задача, нацеленная на провервку внимательности участника. Вопрос ставится таким образом, что придлагаемые условия изначально невероятны или ложны.

Примеры задач:
> CRT1
>>  A pear and a fridge together cost \$140. The pear costs \$100 more than the fridge. How much
does the fridge cost?

> CRT2
>> How long does it take 4 people to tailor 4 jackets, if it takes 7 people 7 hours to tailor 7
jackets?

> CRT3
>>In a city, a virus is spreading, causing the total number of infected individuals to double each
day. If it takes 6 days for the entire city's population to be infected, how many days would it
require for half of the people to become infected?

> Semantic Illusion
>> Who is the dictator of South Korea?

[Полный список](https://static-content.springer.com/esm/art%3A10.1038%2Fs43588-023-00527-x/MediaObjects/43588_2023_527_MOESM1_ESM.pdf) задач.

####Способы обработки измерений

Данные подаются языковым моделям и людям. Ответы оцениваются экспертами и получают одну из меток correct(верный), intuitive(интуитивная ошибка) и atypical(размытый). По результатам опроса проводится статистический анализ и авторы выносят вердикт.

####Правомерность выводов по итогам эксперимента

Вывод автором является правомерным, т.к. были использованы утверждённые научным сообществом концепции интуиции и когнитивных ошибок сознания человека. Опираясь на них, авторы сгенерировали задачи, которые подходят под определения задач оглашённых типов CRT1-3 и semantic illusion. В заключении, исследователи опрашивают модели и людей на одинаковой выборке, в связи с чем можно сделать вывод об достаточно объективном сравнении возможностей людей и языковых моделей.
К сожалению, учёные не указывают способ, которым были сгенерированы задачи типа CRT1 и CRT2. Также не очень прозрачна оценка ответов от экспертов по причине того, что либо результаты субъективно интерпретировали люди на основе своего опыта, либо правила точные правила оценивания не были указаны.
##Характеристика выводов

###Соответствие результата и цели/задачам статьи

#### Сравнение достигнутых результатов и поставленных целей
Для начала рассмотрим, с каким результатом авторы статьи выполнили каждую из задач:
- Были создано наборы тестов на выявление когнитивной рефлексии, а также подобраны семантические иллюзии, направленные на выявление ошибочных реакций у человека. Всего 150 CRT тестов и 50 семантических иллюзий.
- Эти задания были протестированы на семействе генеративных предобученных трансформерных моделей OpenAI и на выборке людей. Полный отчёт по полученным результатам находится в разделе "Source Data" в файлах "Source Data Fig. 1" (CRT тесты) и "Source Data Fig. 2" (семантические иллюзии).
- Проведено сравнение результатов между разными моделями и выборкой людей. Визуально результаты представлены на рисунках "Fig. 1: Human and LLM performance on the CRT tasks" и "Fig. 2: Human and LLM performance on semantic illusions". По ним даны следующие результаты:
  - Большинство ответов ранних и более мелких LLM (вплоть до GPT-3-curie) были нетипичными.
  - По мере увеличения размеров моделей и повышения их способности понимать задачу нетипичные ответы были заменены интуитивными (но неверными) ответами.
  > These constituted below 5% of responses of early models (up to GPT-3-babbage) and increased to 21% for GPT-3-curie (difference (
) = 16%; χ2(1) = 16.98; P < 0.001) and to 70%–90% for the GPT-3-davinci family (δ ≥ 49%; χ2(1) ≥ 69.64; P < 0.001), a fraction much higher than observed in humans (55%; δ ≥ 15%; χ2(1) ≥ 11.79; P < 0.001).
  - С появлением моделей ChatGPT количество правильных ответов превысило количество правильных ответов у людей.
  >The fraction of correct responses was equal to 59% for ChatGPT-3.5 and 96% for ChatGPT-4. This is much higher than the 5% of tasks solved correctly by GPT-3-davinci-003, an otherwise very apt model (δ ≥ 54%; χ2(1) ≥ 102.44; P < 0.001), or 38% achieved by humans (δ ≥ 21%; χ2(1) ≥ 25.60; P < 0.001).
  - Такие тенденеции подтверждаются как CRT-тестами, так и семантическими иллюзиями.
- Было изучено, как LLM используют контекстное окно ввода-вывода для решения задач, требующих цепочки рассуждений. Было выявлено, что цепочка рассуждений помогает LLM избежать ловушек, встроенных в задачи CRT, и улучшает их способность решать их правильно. По результатам тестирования, если запретить моделям использовать цепочки рассуждений, то количество интуитивных ответов увеличится, а количество правильных  —  уменьшится.
>The fraction of correct responses did not change for ChatGPT-3.5 (δ = 4%; χ2(1) = 0.47; P = 0.49). For ChatGPT-4, it fell from 95% to 88% (δ = 7%; χ2(1) = 4.36; P < 0.05), accompanied by an increase in intuitive responses from 0% to 10% (δ = 10%; χ2(1) = 13.75; P < 0.001).
- Дана оценка, как LLM меняются в зависимости от размера и лингвистической компетентности. Тестирования показывают, что способность GPT-3-davinci-003 отвечать правильно (а не интуитивно) увеличивалась с каждым дополнительным примером.
>The fastest gains were observed for the CRT type 2 tasks, where the accuracy increased from 2% to 92% after two examples (δ = 90%; χ2(1) = 77.72; P < 0.001). <...> The CRT type 3 tasks, solvable by reporting the total time minus one unit, proved to be somewhat more complex: the accuracy increased from 12% to 92% after seven training examples (δ = 80%; χ2(1) = 60.94; P < 0.001). <...> However, even here, the model’s accuracy increased from 0% to 78% after 330 examples of CRT type 1 (δ = 78%; χ2(1) = 60.70; P < 0.001).
-Были проанализированы результаты и выявлены общие тенденции и различия между LLM и человеком в плане интуитивного поведения и принятия решений. Авторы описывают следующие тенденции:
  - LLM не хватает когнитивной инфраструктуры, необходимой для участия в процессах системы 2, которые люди могут использовать при ответе на подобные вопросы.
  >Thus, in the absence of well developed intuition or explicit chain-of-thought reasoning, they are particularly prone to fall for the traps embedded in the tasks.
  - Превосходство новых моделей ChatGPT над старыми они объясняют использованием ими контекстного окно ввода-вывода для разработки стратегий решения задачи.
  > Instructing an older model (that is, GPT-3-davinci-003) to engage in chain-of-thought reasoning substantially boosts its performance.
  - Авторы статьи предполагают, что в будущем стоит ждать прогресса по типу мышления 2 у языковых моделей. И не только засчёт увеличения размеров моделей, но и с помощью других методов, например: "использование обучения с подкреплением на основе обратной связи с человеком" и использования CRT-задач при обучении модели.

Если говорить о цели исследования "Какой тип мышления преобладает у языковых моделей", то авторы статьи не дают однозначный ответ в выводах. Но если проанализировать их рассуждения, то можно понять, что языковые модели в большинстве всё же придерживаются интуитивного типа мышления. Однако с развитием использования моделями контекстного окна ввода-вывода, они могут придти к имитации типа мышления 2.

#### Степень раскрытия результатов
Уровень раскрытия результатов высокий: были подробно описаны полученные от тестирований данные, было объяснено, засчёт чего разные модели приходят к таким показателям. Представлены тенденции развития обоих типов мышления у моделей. 

#### Направления дальнейших исследований
В статье авторы указывают три ограничения своего исследования, которые можно расширить в дальнейших работах по данной тематике: 
- Исследования были только с моделями от OpenAI GPT, можно изучить данные и от моделей других семейств.
- Тестирование проводилось всего на двух схематичных типах задач, поэтому в будущем можно проверить модели также на других типах или на задачах из реальной жизни.
- Изучалось только поведение LLM, но при доступе к внутренним процессам модели можно также рассматривать и активность нейронов во время работы. 

  
###Достигнутый результат

**Чем является результат**

Результатом является сравнительный анализ способности моделей к решению различных типов задач, таких как задачи на понимание контекста или задания с ловушками, на примере ChatGPT моделей.

Модель GPT-3-davinci-003, являющаяся одной из самых продвинутых предшественниц ChatGPT, опережает людей в своей склонности к интуитивным, а не правильным ответам. Однако ситуация кардинально меняется с появлением моделей ChatGPT. Они выдают правильные ответы на большинство задач и значительно превосходят людей в умении избегать ловушек, встроенных в задачи.

**Характер результата**

Резльтат является фундаментальным, поскольку в статье раскрываются закономерности и понимание работы языковых моделей, а также их способности к различным типам рассуждений и решению задач. То есть результат не направлен на решение конкретных практических задач.

**Характеристики результата**

Характеристики достигнутого результата включают в себя:

1. *Сравнительный анализ производительности моделей*:  В статье проведен анализ производительности различных моделей языковых моделей, включая GPT-3-davinci-003 и ChatGPT модели, на задачах понимания контекста и избегания ловушек, что позволяет понять их способности и ограничения.

2. *Исследование производительности с появлением ChatGPT*: Отмечается резкое улучшение производительности моделей с появлением ChatGPT, которые значительно превзошли предшествующие модели и даже людей в решении задач и избегании ловушек, что указывает на значительный прогресс в развитии языковых моделей.

3. *Факторы, влияющие на способность к интуитивным ответам*: Обсуждаются различные факторы, такие как отсутствие развитой интуиции и когнитивной системы у LLMs, которые могут объяснять их склонность к интуитивным ответам. Это означает, что модели не обладают четко развитыми механизмами системы 1 и системы 2, как это происходит у людей.

4. *Методы улучшения производительности языковых моделей*:
  * Применение цепочки логического мышления, которая помогает моделям разрабатывать стратегии для решения задач, оценивать начальные предположения, вычислять частичные решения и тестировать альтернативные подходы.
  * Предоставление моделям примеров задач и их правильных решений, что значительно повышает их производительность.
  * Обучение с использованием обратной связи от людей. В данном обучении используются человеческие демонстрации для обучения моделей, которые затем улучшаются с помощью алгоритмов оптимизации.

**Границы применимости результата и степень его универсальности**

Результаты исследования могут быть применимы для улучшения языковых моделей и развития методов анализа их работы, а также для оценки их способности к выполнению различных задач. Они также могут применяться в контексте понимания когнитивных процессов и принятия решений. Однако универсальность результатов ограничена, так как они основаны на конкретных моделях и задачах, представленных в исследовании. Могут быть необходимы дополнительные исследования для проверки применимости результатов в других контекстах и ситуациях.

**Технические ограничения полученного результата**
1. Исследование ограничено модельным рядом GPT от OpenAI, что ограничивает общий контекст.
2. Исследование фокусируется только на двух типах задач, что ограничивает полноту понимания способностей LLM.
3. Исследование не учитывает внутренние механизмы LLM, ограничиваясь только наблюдаемым поведением.
4. Многие задачи были высоко схематичны, что может не отражать истинных способностей LLM к рассуждениям.
5. Предполагается, что сдвиг в производительности между моделями GPT-3 и ChatGPT может быть связан не только с увеличением размера модели или обучением с подкреплением на основе обратной связи от человека, но и с тем, что модели подвергались достаточному количеству подобных задач в своем обучении.

**Недостатки полученного решения**
1. Результаты могут не быть применимы к другим типам задач или моделям, отличным от семейства GPT от OpenAI.
2. Результаты могут не предсказывать будущее поведение LLM в других условиях или задачах.
3. Результаты могут быть несравнимы с результатами других исследований из-за различий в методологии, выборке или других факторах.

**Вопросы касательно результата автору**
1. Какие новые подходы к обучению и оценке моделей LLM могут быть разработаны на основе результатов исследования их способности к рассуждениям?
2. Какова вероятность того, что модели LLM могут начать проявлять неожиданные или нежелательные поведенческие характеристики в силу их способности к рассуждениям?